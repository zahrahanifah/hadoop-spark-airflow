[2023-03-02 14:28:13,347] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: avg_product_price.spark_submit manual__2023-03-02T14:22:09.641171+00:00 [queued]>
[2023-03-02 14:28:13,384] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: avg_product_price.spark_submit manual__2023-03-02T14:22:09.641171+00:00 [queued]>
[2023-03-02 14:28:13,385] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-03-02 14:28:13,386] {taskinstance.py:1357} INFO - Starting attempt 2 of 2
[2023-03-02 14:28:13,387] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-03-02 14:28:13,438] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): spark_submit> on 2023-03-02 14:22:09.641171+00:00
[2023-03-02 14:28:13,446] {standard_task_runner.py:52} INFO - Started process 478 to run task
[2023-03-02 14:28:13,454] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'avg_product_price', 'spark_submit', 'manual__2023-03-02T14:22:09.641171+00:00', '--job-id', '45', '--raw', '--subdir', 'DAGS_FOLDER/avg_product_price.py', '--cfg-path', '/tmp/tmpwckh4xa7', '--error-file', '/tmp/tmpc1qulwcw']
[2023-03-02 14:28:13,458] {standard_task_runner.py:80} INFO - Job 45: Subtask spark_submit
[2023-03-02 14:28:13,611] {task_command.py:370} INFO - Running <TaskInstance: avg_product_price.spark_submit manual__2023-03-02T14:22:09.641171+00:00 [running]> on host 1e8147149b8a
[2023-03-02 14:28:13,973] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=ayyoub
AIRFLOW_CTX_DAG_ID=avg_product_price
AIRFLOW_CTX_TASK_ID=spark_submit
AIRFLOW_CTX_EXECUTION_DATE=2023-03-02T14:22:09.641171+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=manual__2023-03-02T14:22:09.641171+00:00
[2023-03-02 14:28:14,009] {base.py:68} INFO - Using connection ID 'spark-hadoop' for task execution.
[2023-03-02 14:28:14,012] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local[*] --name arrow-spark /hadoop-data/map_reduce/spark/average_price.py
[2023-03-02 14:28:30,841] {spark_submit.py:495} INFO - 23/03/02 14:28:30 INFO SparkContext: Running Spark version 3.3.2
[2023-03-02 14:28:31,594] {spark_submit.py:495} INFO - 23/03/02 14:28:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-03-02 14:28:32,148] {spark_submit.py:495} INFO - 23/03/02 14:28:32 INFO ResourceUtils: ==============================================================
[2023-03-02 14:28:32,149] {spark_submit.py:495} INFO - 23/03/02 14:28:32 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-03-02 14:28:32,151] {spark_submit.py:495} INFO - 23/03/02 14:28:32 INFO ResourceUtils: ==============================================================
[2023-03-02 14:28:32,153] {spark_submit.py:495} INFO - 23/03/02 14:28:32 INFO SparkContext: Submitted application: average_product_price
[2023-03-02 14:28:32,284] {spark_submit.py:495} INFO - 23/03/02 14:28:32 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-03-02 14:28:32,353] {spark_submit.py:495} INFO - 23/03/02 14:28:32 INFO ResourceProfile: Limiting resource is cpu
[2023-03-02 14:28:32,355] {spark_submit.py:495} INFO - 23/03/02 14:28:32 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-03-02 14:28:32,695] {spark_submit.py:495} INFO - 23/03/02 14:28:32 INFO SecurityManager: Changing view acls to: ***
[2023-03-02 14:28:32,696] {spark_submit.py:495} INFO - 23/03/02 14:28:32 INFO SecurityManager: Changing modify acls to: ***
[2023-03-02 14:28:32,697] {spark_submit.py:495} INFO - 23/03/02 14:28:32 INFO SecurityManager: Changing view acls groups to:
[2023-03-02 14:28:32,698] {spark_submit.py:495} INFO - 23/03/02 14:28:32 INFO SecurityManager: Changing modify acls groups to:
[2023-03-02 14:28:32,700] {spark_submit.py:495} INFO - 23/03/02 14:28:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-03-02 14:28:34,072] {spark_submit.py:495} INFO - 23/03/02 14:28:34 INFO Utils: Successfully started service 'sparkDriver' on port 38967.
[2023-03-02 14:28:34,432] {spark_submit.py:495} INFO - 23/03/02 14:28:34 INFO SparkEnv: Registering MapOutputTracker
[2023-03-02 14:28:34,684] {spark_submit.py:495} INFO - 23/03/02 14:28:34 INFO SparkEnv: Registering BlockManagerMaster
[2023-03-02 14:28:34,815] {spark_submit.py:495} INFO - 23/03/02 14:28:34 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-03-02 14:28:34,817] {spark_submit.py:495} INFO - 23/03/02 14:28:34 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-03-02 14:28:34,840] {spark_submit.py:495} INFO - 23/03/02 14:28:34 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-03-02 14:28:35,005] {spark_submit.py:495} INFO - 23/03/02 14:28:35 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-731484fc-6937-4ca3-a6b0-ef6d219f4fe1
[2023-03-02 14:28:35,129] {spark_submit.py:495} INFO - 23/03/02 14:28:35 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-03-02 14:28:35,243] {spark_submit.py:495} INFO - 23/03/02 14:28:35 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-03-02 14:28:36,438] {spark_submit.py:495} INFO - 23/03/02 14:28:36 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-03-02 14:28:36,977] {spark_submit.py:495} INFO - 23/03/02 14:28:36 INFO Executor: Starting executor ID driver on host 1e8147149b8a
[2023-03-02 14:28:37,012] {spark_submit.py:495} INFO - 23/03/02 14:28:37 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-03-02 14:28:37,112] {spark_submit.py:495} INFO - 23/03/02 14:28:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32903.
[2023-03-02 14:28:37,114] {spark_submit.py:495} INFO - 23/03/02 14:28:37 INFO NettyBlockTransferService: Server created on 1e8147149b8a:32903
[2023-03-02 14:28:37,127] {spark_submit.py:495} INFO - 23/03/02 14:28:37 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-03-02 14:28:37,193] {spark_submit.py:495} INFO - 23/03/02 14:28:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1e8147149b8a, 32903, None)
[2023-03-02 14:28:37,222] {spark_submit.py:495} INFO - 23/03/02 14:28:37 INFO BlockManagerMasterEndpoint: Registering block manager 1e8147149b8a:32903 with 434.4 MiB RAM, BlockManagerId(driver, 1e8147149b8a, 32903, None)
[2023-03-02 14:28:37,234] {spark_submit.py:495} INFO - 23/03/02 14:28:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 1e8147149b8a, 32903, None)
[2023-03-02 14:28:37,242] {spark_submit.py:495} INFO - 23/03/02 14:28:37 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 1e8147149b8a, 32903, None)
[2023-03-02 14:28:38,563] {spark_submit.py:495} INFO - /opt/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/sql/context.py:114: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.
[2023-03-02 14:28:41,082] {spark_submit.py:495} INFO - 23/03/02 14:28:41 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-03-02 14:28:41,276] {spark_submit.py:495} INFO - 23/03/02 14:28:41 INFO SharedState: Warehouse path is 'file:/home/***/spark-warehouse'.
[2023-03-02 14:28:52,915] {spark_submit.py:495} INFO - 23/03/02 14:28:52 INFO InMemoryFileIndex: It took 756 ms to list leaf files for 1 paths.
[2023-03-02 14:28:53,265] {spark_submit.py:495} INFO - 23/03/02 14:28:53 INFO InMemoryFileIndex: It took 26 ms to list leaf files for 1 paths.
[2023-03-02 14:29:08,548] {spark_submit.py:495} INFO - 23/03/02 14:29:08 INFO FileSourceStrategy: Pushed Filters:
[2023-03-02 14:29:08,562] {spark_submit.py:495} INFO - 23/03/02 14:29:08 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
[2023-03-02 14:29:08,580] {spark_submit.py:495} INFO - 23/03/02 14:29:08 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-03-02 14:34:07,706] {spark_submit.py:495} INFO - 23/03/02 14:34:05 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map()) by listener AppStatusListener took 3.218350562s.
[2023-03-02 14:34:10,777] {spark_submit.py:495} INFO - 23/03/02 14:34:07 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@c947623)) by listener ExecutionListenerBus took 5.737709484s.
[2023-03-02 14:34:13,793] {spark_submit.py:495} INFO - 23/03/02 14:34:07 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@c947623)) by listener HeartbeatReceiver took 6.294962002s.
[2023-03-02 14:34:18,409] {spark_submit.py:495} INFO - 23/03/02 14:34:14 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.TimeoutException: Cannot receive any reply from 1e8147149b8a:38967 in 10000 milliseconds
[2023-03-02 14:34:33,667] {spark_submit.py:495} INFO - 23/03/02 14:34:29 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@c947623)) by listener AppStatusListener took 3.149948734s.
[2023-03-02 14:34:57,490] {spark_submit.py:495} INFO - 23/03/02 14:34:55 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@42308858)) by listener AppStatusListener took 2.736676986s.
[2023-03-02 14:34:57,997] {spark_submit.py:495} INFO - 23/03/02 14:34:55 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 130196 ms exceeds timeout 120000 ms
[2023-03-02 14:35:10,214] {spark_submit.py:495} INFO - 23/03/02 14:35:08 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@70bbf869)) by listener AppStatusListener took 1.5460301s.
[2023-03-02 14:35:20,592] {spark_submit.py:495} INFO - 23/03/02 14:35:19 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@325def6c)) by listener AppStatusListener took 5.296338638s.
[2023-03-02 14:35:20,872] {spark_submit.py:495} INFO - 23/03/02 14:35:08 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-02 14:35:20,873] {spark_submit.py:495} INFO - org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval
[2023-03-02 14:35:20,874] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
[2023-03-02 14:35:20,874] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
[2023-03-02 14:35:20,875] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
[2023-03-02 14:35:20,876] {spark_submit.py:495} INFO - at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
[2023-03-02 14:35:20,877] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
[2023-03-02 14:35:20,878] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-02 14:35:20,878] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1053)
[2023-03-02 14:35:20,879] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-02 14:35:20,879] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:20,880] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-02 14:35:20,881] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-02 14:35:20,882] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:20,883] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-02 14:35:20,883] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-02 14:35:20,884] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-02 14:35:20,885] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-02 14:35:20,886] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-02 14:35:20,886] {spark_submit.py:495} INFO - Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]
[2023-03-02 14:35:20,887] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)
[2023-03-02 14:35:20,887] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)
[2023-03-02 14:35:20,888] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:293)
[2023-03-02 14:35:20,889] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:20,889] {spark_submit.py:495} INFO - ... 12 more
[2023-03-02 14:35:28,532] {spark_submit.py:495} INFO - 23/03/02 14:35:27 WARN NettyRpcEnv: Ignored message: true
[2023-03-02 14:35:29,567] {spark_submit.py:495} INFO - 23/03/02 14:35:28 WARN NettyRpcEnv: Ignored message: true
[2023-03-02 14:35:32,804] {spark_submit.py:495} INFO - 23/03/02 14:35:32 WARN NettyRpcEnv: Ignored message: HeartbeatResponse(true)
[2023-03-02 14:35:32,978] {spark_submit.py:495} INFO - 23/03/02 14:35:32 WARN SparkContext: Killing executors is not supported by current scheduler.
[2023-03-02 14:35:33,351] {spark_submit.py:495} INFO - 23/03/02 14:35:33 INFO Executor: Told to re-register on heartbeat
[2023-03-02 14:35:34,468] {spark_submit.py:495} INFO - 23/03/02 14:35:34 INFO BlockManager: BlockManager BlockManagerId(driver, 1e8147149b8a, 32903, None) re-registering with master
[2023-03-02 14:35:34,613] {spark_submit.py:495} INFO - 23/03/02 14:35:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1e8147149b8a, 32903, None)
[2023-03-02 14:35:37,420] {spark_submit.py:495} INFO - 23/03/02 14:35:37 ERROR Inbox: Ignoring error
[2023-03-02 14:35:37,486] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:37,486] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:37,487] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:37,488] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-02 14:35:37,488] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-02 14:35:37,489] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-02 14:35:37,490] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-02 14:35:37,491] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-02 14:35:37,491] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-02 14:35:37,492] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-02 14:35:37,493] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-02 14:35:37,493] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-02 14:35:37,494] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:37,495] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:37,495] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:37,496] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:37,496] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:37,497] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:37,498] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-02 14:35:37,499] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-02 14:35:37,500] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-02 14:35:37,500] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-02 14:35:37,501] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1e8147149b8a:38967
[2023-03-02 14:35:37,502] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-02 14:35:37,503] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-02 14:35:37,503] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-02 14:35:37,504] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-02 14:35:37,505] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:37,505] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:37,506] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:37,507] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:37,507] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:37,508] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:37,509] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:37,509] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:37,510] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:37,511] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:37,511] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:37,512] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:37,513] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:37,513] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:37,514] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:37,515] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:37,516] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:37,517] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:37,517] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:37,518] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:37,519] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:37,520] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:37,521] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:37,521] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-02 14:35:37,522] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-02 14:35:37,523] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:37,523] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-02 14:35:37,524] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-02 14:35:37,524] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-02 14:35:37,525] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-02 14:35:37,526] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-02 14:35:37,526] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-02 14:35:37,527] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:37,528] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:37,528] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:37,529] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:37,529] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-02 14:35:37,530] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-02 14:35:37,531] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-02 14:35:37,532] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-02 14:35:37,533] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-02 14:35:37,533] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-02 14:35:37,534] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:37,535] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:37,536] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:37,536] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:37,537] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:37,538] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:37,538] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:37,539] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:37,540] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:37,541] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:37,541] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-02 14:35:37,542] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-02 14:35:37,543] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-02 14:35:37,543] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-02 14:35:37,544] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-02 14:35:37,545] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-02 14:35:37,545] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:37,546] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:37,547] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:37,547] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:37,548] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:37,549] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 14:35:37,550] {spark_submit.py:495} INFO - 23/03/02 14:35:36 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-02 14:35:37,551] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:37,552] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:37,552] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:37,553] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-02 14:35:37,554] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-02 14:35:37,555] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-02 14:35:37,555] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-02 14:35:37,556] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-02 14:35:37,557] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-02 14:35:37,557] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:37,558] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-02 14:35:37,559] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-02 14:35:37,560] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:37,560] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-02 14:35:37,561] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-02 14:35:37,562] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-02 14:35:37,562] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-02 14:35:37,563] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-02 14:35:37,564] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:37,565] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:37,566] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:37,566] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-02 14:35:37,567] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-02 14:35:37,568] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-02 14:35:37,569] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-02 14:35:37,570] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-02 14:35:37,570] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-02 14:35:37,571] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-02 14:35:37,572] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-02 14:35:37,572] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-02 14:35:37,573] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:37,574] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:37,574] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:37,575] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:37,576] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:37,576] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:37,577] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-02 14:35:37,578] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 14:35:37,578] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1e8147149b8a:38967
[2023-03-02 14:35:37,579] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-02 14:35:37,580] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-02 14:35:37,580] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-02 14:35:37,581] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-02 14:35:37,582] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:37,583] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:37,584] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:37,584] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:37,585] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:37,586] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:37,587] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:37,587] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:37,588] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:37,589] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:37,589] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:37,590] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:37,591] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:37,591] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:37,592] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:37,593] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:37,593] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:37,594] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:37,595] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:37,595] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:37,596] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:37,597] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:37,598] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:37,599] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-02 14:35:37,600] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-02 14:35:37,600] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:37,601] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-02 14:35:37,602] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-02 14:35:37,603] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-02 14:35:37,603] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-02 14:35:37,604] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-02 14:35:37,605] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-02 14:35:37,606] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:37,606] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:37,607] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:37,608] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:37,609] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-02 14:35:37,609] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-02 14:35:37,610] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-02 14:35:37,611] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-02 14:35:37,611] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-02 14:35:37,612] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-02 14:35:37,613] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:37,614] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:37,615] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:37,616] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:37,616] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:37,617] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:37,618] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:37,619] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:37,619] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:37,620] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:37,621] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-02 14:35:37,622] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-02 14:35:37,622] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-02 14:35:37,623] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-02 14:35:37,624] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-02 14:35:37,625] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-02 14:35:37,625] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:37,626] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:37,627] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:37,627] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:37,628] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:37,629] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 14:35:37,694] {spark_submit.py:495} INFO - 23/03/02 14:35:37 INFO Executor: Told to re-register on heartbeat
[2023-03-02 14:35:37,695] {spark_submit.py:495} INFO - 23/03/02 14:35:37 INFO BlockManager: BlockManager BlockManagerId(driver, 1e8147149b8a, 32903, None) re-registering with master
[2023-03-02 14:35:37,696] {spark_submit.py:495} INFO - 23/03/02 14:35:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1e8147149b8a, 32903, None)
[2023-03-02 14:35:37,885] {spark_submit.py:495} INFO - 23/03/02 14:35:37 ERROR Inbox: Ignoring error
[2023-03-02 14:35:37,886] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:37,887] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:37,888] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:37,889] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-02 14:35:37,890] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-02 14:35:37,891] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-02 14:35:37,892] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-02 14:35:37,893] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-02 14:35:37,894] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-02 14:35:37,895] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-02 14:35:37,896] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-02 14:35:37,897] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-02 14:35:37,898] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:37,899] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:37,900] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:37,901] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:37,902] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:37,903] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:37,904] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-02 14:35:37,905] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-02 14:35:37,906] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-02 14:35:37,907] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-02 14:35:37,908] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1e8147149b8a:38967
[2023-03-02 14:35:37,909] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-02 14:35:37,909] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-02 14:35:37,910] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-02 14:35:37,911] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-02 14:35:37,912] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:37,913] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:37,913] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:37,915] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:37,916] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:37,916] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:37,917] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:37,918] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:37,919] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:37,920] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:37,921] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:37,921] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:37,923] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:37,923] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:37,924] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:37,925] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:37,926] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:37,927] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:37,928] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:37,930] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:37,930] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:37,932] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:37,933] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:37,933] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-02 14:35:37,934] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-02 14:35:37,935] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:37,936] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-02 14:35:37,937] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-02 14:35:37,938] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-02 14:35:37,939] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-02 14:35:37,940] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-02 14:35:37,941] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-02 14:35:37,942] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:37,942] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:37,943] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:37,944] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:37,945] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-02 14:35:37,945] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-02 14:35:37,946] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-02 14:35:37,952] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-02 14:35:37,955] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-02 14:35:37,958] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-02 14:35:37,960] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:37,962] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:37,964] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:37,965] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:37,967] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:37,968] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:37,970] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:37,971] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:37,972] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:37,973] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:37,974] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-02 14:35:37,975] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-02 14:35:37,976] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-02 14:35:37,977] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-02 14:35:37,978] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-02 14:35:37,979] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-02 14:35:37,980] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:37,981] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:37,982] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:37,983] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:37,984] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:37,985] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 14:35:37,986] {spark_submit.py:495} INFO - 23/03/02 14:35:37 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-02 14:35:37,987] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:37,988] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:37,989] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:37,990] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-02 14:35:37,991] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-02 14:35:37,992] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-02 14:35:37,993] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-02 14:35:37,995] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-02 14:35:37,996] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-02 14:35:37,997] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:37,999] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-02 14:35:38,000] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-02 14:35:38,000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:38,001] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-02 14:35:38,002] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-02 14:35:38,003] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-02 14:35:38,004] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-02 14:35:38,005] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-02 14:35:38,006] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:38,007] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:38,008] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:38,008] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-02 14:35:38,009] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-02 14:35:38,010] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-02 14:35:38,011] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-02 14:35:38,012] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-02 14:35:38,013] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-02 14:35:38,013] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-02 14:35:38,015] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-02 14:35:38,016] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-02 14:35:38,017] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:38,018] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:38,018] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:38,019] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:38,020] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:38,021] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:38,021] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-02 14:35:38,022] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 14:35:38,023] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1e8147149b8a:38967
[2023-03-02 14:35:38,023] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-02 14:35:38,024] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-02 14:35:38,024] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-02 14:35:38,025] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-02 14:35:38,026] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:38,027] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:38,027] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:38,028] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:38,029] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:38,029] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:38,030] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:38,031] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:38,032] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:38,033] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:38,034] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:38,035] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:38,035] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:38,037] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:38,038] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:38,038] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:38,039] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:38,040] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:38,041] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:38,042] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:38,043] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:38,044] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:38,045] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:38,045] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-02 14:35:38,046] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-02 14:35:38,047] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:38,048] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-02 14:35:38,049] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-02 14:35:38,050] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-02 14:35:38,050] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-02 14:35:38,055] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-02 14:35:38,056] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-02 14:35:38,057] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:38,058] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:38,059] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:38,060] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:38,061] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-02 14:35:38,062] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-02 14:35:38,062] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-02 14:35:38,063] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-02 14:35:38,064] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-02 14:35:38,065] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-02 14:35:38,066] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:38,067] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:38,068] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:38,068] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:38,069] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:38,070] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:38,070] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:38,071] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:38,071] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:38,072] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:38,072] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-02 14:35:38,073] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-02 14:35:38,074] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-02 14:35:38,074] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-02 14:35:38,075] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-02 14:35:38,076] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-02 14:35:38,076] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:38,077] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:38,077] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:38,078] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:38,079] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:38,079] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 14:35:38,081] {spark_submit.py:495} INFO - 23/03/02 14:35:37 INFO Executor: Told to re-register on heartbeat
[2023-03-02 14:35:38,082] {spark_submit.py:495} INFO - 23/03/02 14:35:37 INFO BlockManager: BlockManager BlockManagerId(driver, 1e8147149b8a, 32903, None) re-registering with master
[2023-03-02 14:35:38,083] {spark_submit.py:495} INFO - 23/03/02 14:35:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1e8147149b8a, 32903, None)
[2023-03-02 14:35:38,083] {spark_submit.py:495} INFO - 23/03/02 14:35:37 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-02 14:35:38,084] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:38,085] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:38,085] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:38,086] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-02 14:35:38,087] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-02 14:35:38,087] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-02 14:35:38,088] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-02 14:35:38,089] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-02 14:35:38,090] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-02 14:35:38,090] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:38,091] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-02 14:35:38,092] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-02 14:35:38,092] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:38,093] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-02 14:35:38,094] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-02 14:35:38,094] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-02 14:35:38,095] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-02 14:35:38,096] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-02 14:35:38,096] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:38,098] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:38,099] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:38,099] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-02 14:35:38,100] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-02 14:35:38,101] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-02 14:35:38,102] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-02 14:35:38,103] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-02 14:35:38,103] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-02 14:35:38,104] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-02 14:35:38,105] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-02 14:35:38,106] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-02 14:35:38,107] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:38,108] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:38,109] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:38,109] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:38,110] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:38,111] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:38,112] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-02 14:35:38,113] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 14:35:38,113] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1e8147149b8a:38967
[2023-03-02 14:35:38,114] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-02 14:35:38,115] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-02 14:35:38,115] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-02 14:35:38,116] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-02 14:35:38,117] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:38,117] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:38,118] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:38,118] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:38,119] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:38,119] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:38,120] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:38,120] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:38,121] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:38,121] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:38,122] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:38,123] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:38,133] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:38,134] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:38,136] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:38,138] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:38,139] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:38,141] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:38,142] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:38,144] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:38,145] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:38,146] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:38,147] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:38,149] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-02 14:35:38,150] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-02 14:35:38,151] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:38,152] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-02 14:35:38,153] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-02 14:35:38,154] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-02 14:35:38,154] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-02 14:35:38,155] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-02 14:35:38,156] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-02 14:35:38,156] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:38,157] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:38,157] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:38,158] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:38,159] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-02 14:35:38,159] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-02 14:35:38,160] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-02 14:35:38,161] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-02 14:35:38,161] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-02 14:35:38,162] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-02 14:35:38,162] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:38,163] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:38,163] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:38,164] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:38,165] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:38,166] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:38,167] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:38,167] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:38,168] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:38,168] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:38,169] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-02 14:35:38,170] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-02 14:35:38,171] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-02 14:35:38,172] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-02 14:35:38,173] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-02 14:35:38,173] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-02 14:35:38,174] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:38,175] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:38,175] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:38,176] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:38,176] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:38,177] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 14:35:38,177] {spark_submit.py:495} INFO - 23/03/02 14:35:37 ERROR Inbox: Ignoring error
[2023-03-02 14:35:38,178] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:38,178] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:38,179] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:38,179] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-02 14:35:38,180] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-02 14:35:38,181] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-02 14:35:38,182] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-02 14:35:38,182] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-02 14:35:38,183] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-02 14:35:38,184] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-02 14:35:38,184] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-02 14:35:38,185] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-02 14:35:38,186] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:38,186] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:38,187] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:38,187] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:38,188] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:38,188] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:38,189] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-02 14:35:38,189] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-02 14:35:38,190] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-02 14:35:38,190] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-02 14:35:38,191] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1e8147149b8a:38967
[2023-03-02 14:35:38,191] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-02 14:35:38,192] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-02 14:35:38,192] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-02 14:35:38,193] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-02 14:35:38,193] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:38,194] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:38,194] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:38,194] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:38,195] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:38,195] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:38,196] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:38,196] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:38,197] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:38,197] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:38,198] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:38,199] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:38,200] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:38,200] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:38,201] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:38,201] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:38,202] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:38,202] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:38,203] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:38,203] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:38,204] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:38,204] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:38,205] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:38,205] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-02 14:35:38,206] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-02 14:35:38,206] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:38,207] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-02 14:35:38,207] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-02 14:35:38,207] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-02 14:35:38,208] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-02 14:35:38,208] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-02 14:35:38,209] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-02 14:35:38,209] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:38,210] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:38,210] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:38,211] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:38,211] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-02 14:35:38,212] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-02 14:35:38,212] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-02 14:35:38,213] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-02 14:35:38,213] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-02 14:35:38,214] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-02 14:35:38,215] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:38,215] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:38,216] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:38,217] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:38,217] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:38,218] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:38,218] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:38,219] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:38,219] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:38,220] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:38,220] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-02 14:35:38,221] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-02 14:35:38,221] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-02 14:35:38,222] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-02 14:35:38,222] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-02 14:35:38,223] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-02 14:35:38,223] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:38,223] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:38,224] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:38,224] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:38,225] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:38,225] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 14:35:38,226] {spark_submit.py:495} INFO - 23/03/02 14:35:37 INFO Executor: Told to re-register on heartbeat
[2023-03-02 14:35:38,226] {spark_submit.py:495} INFO - 23/03/02 14:35:37 INFO BlockManager: BlockManager BlockManagerId(driver, 1e8147149b8a, 32903, None) re-registering with master
[2023-03-02 14:35:38,226] {spark_submit.py:495} INFO - 23/03/02 14:35:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1e8147149b8a, 32903, None)
[2023-03-02 14:35:38,227] {spark_submit.py:495} INFO - 23/03/02 14:35:37 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-02 14:35:38,227] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:38,228] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:38,228] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:38,229] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-02 14:35:38,229] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-02 14:35:38,229] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-02 14:35:38,230] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-02 14:35:38,231] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-02 14:35:38,232] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-02 14:35:38,233] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:38,233] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-02 14:35:38,234] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-02 14:35:38,234] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:38,235] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-02 14:35:38,235] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-02 14:35:38,236] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-02 14:35:38,236] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-02 14:35:38,237] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-02 14:35:38,238] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:38,238] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:38,239] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:38,239] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-02 14:35:38,240] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-02 14:35:38,240] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-02 14:35:38,241] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-02 14:35:38,241] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-02 14:35:38,242] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-02 14:35:38,242] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-02 14:35:38,243] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-02 14:35:38,243] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-02 14:35:38,244] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:38,244] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:38,245] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:38,245] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:38,246] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:38,246] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:38,247] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-02 14:35:38,248] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 14:35:38,249] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1e8147149b8a:38967
[2023-03-02 14:35:38,250] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-02 14:35:38,250] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-02 14:35:38,251] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-02 14:35:38,252] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-02 14:35:38,252] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:38,253] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:38,253] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:38,254] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:38,254] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:38,255] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:38,255] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:38,256] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:38,257] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:38,257] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:38,258] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:38,258] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:38,259] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:38,259] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:38,260] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:38,260] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:38,261] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:38,261] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:38,262] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:38,262] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:38,263] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:38,263] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:38,264] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:38,265] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-02 14:35:38,266] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-02 14:35:38,267] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:38,268] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-02 14:35:38,269] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-02 14:35:38,269] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-02 14:35:38,270] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-02 14:35:38,270] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-02 14:35:38,271] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-02 14:35:38,272] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:38,272] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:38,273] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:38,273] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:38,274] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-02 14:35:38,274] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-02 14:35:38,275] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-02 14:35:38,275] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-02 14:35:38,276] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-02 14:35:38,276] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-02 14:35:38,277] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:38,278] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:38,278] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:38,279] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:38,279] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:38,280] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:38,281] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:38,282] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:38,283] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:38,283] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:38,284] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-02 14:35:38,285] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-02 14:35:38,286] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-02 14:35:38,287] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-02 14:35:38,287] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-02 14:35:38,288] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-02 14:35:38,288] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:38,289] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:38,290] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:38,290] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:38,291] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:38,292] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 14:35:38,292] {spark_submit.py:495} INFO - 23/03/02 14:35:37 ERROR Inbox: Ignoring error
[2023-03-02 14:35:38,293] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:38,294] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:38,294] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:38,295] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-02 14:35:38,295] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-02 14:35:38,296] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-02 14:35:38,296] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-02 14:35:38,297] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-02 14:35:38,298] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-02 14:35:38,299] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-02 14:35:38,300] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-02 14:35:38,301] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-02 14:35:38,302] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:38,303] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:38,303] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:38,304] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:38,305] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:38,305] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:38,306] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-02 14:35:38,306] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-02 14:35:38,307] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-02 14:35:38,308] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-02 14:35:38,308] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1e8147149b8a:38967
[2023-03-02 14:35:38,309] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-02 14:35:38,309] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-02 14:35:38,310] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-02 14:35:38,310] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-02 14:35:38,311] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:38,312] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:38,312] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:38,313] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:38,313] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:38,314] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:38,315] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:38,316] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:38,317] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:38,317] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:38,318] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:38,319] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:38,319] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:38,320] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:38,320] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:38,321] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:38,322] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:38,322] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:38,323] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:38,323] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:38,324] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:38,325] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:38,325] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:38,326] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-02 14:35:38,326] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-02 14:35:38,327] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:38,327] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-02 14:35:38,328] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-02 14:35:38,328] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-02 14:35:38,329] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-02 14:35:38,329] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-02 14:35:38,330] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-02 14:35:38,331] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:38,332] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:38,333] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:38,334] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:38,334] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-02 14:35:38,335] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-02 14:35:38,336] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-02 14:35:38,336] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-02 14:35:38,337] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-02 14:35:38,337] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-02 14:35:38,338] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:38,338] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:38,339] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:38,340] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:38,340] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:38,341] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:38,341] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:38,342] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:38,342] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:38,343] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:38,343] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-02 14:35:38,344] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-02 14:35:38,345] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-02 14:35:38,345] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-02 14:35:38,346] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-02 14:35:38,346] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-02 14:35:38,347] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:38,348] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:38,349] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:38,350] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:38,350] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:38,351] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 14:35:38,352] {spark_submit.py:495} INFO - 23/03/02 14:35:37 INFO Executor: Told to re-register on heartbeat
[2023-03-02 14:35:38,352] {spark_submit.py:495} INFO - 23/03/02 14:35:37 INFO BlockManager: BlockManager BlockManagerId(driver, 1e8147149b8a, 32903, None) re-registering with master
[2023-03-02 14:35:38,353] {spark_submit.py:495} INFO - 23/03/02 14:35:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1e8147149b8a, 32903, None)
[2023-03-02 14:35:38,354] {spark_submit.py:495} INFO - 23/03/02 14:35:37 ERROR Inbox: Ignoring error
[2023-03-02 14:35:38,354] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:38,355] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:38,355] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:38,356] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-02 14:35:38,357] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-02 14:35:38,357] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-02 14:35:38,358] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-02 14:35:38,358] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-02 14:35:38,359] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-02 14:35:38,359] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-02 14:35:38,360] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-02 14:35:38,361] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-02 14:35:38,362] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:38,362] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:38,363] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:38,364] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:38,365] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:38,366] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:38,367] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-02 14:35:38,367] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-02 14:35:38,368] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-02 14:35:38,369] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-02 14:35:38,369] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1e8147149b8a:38967
[2023-03-02 14:35:38,370] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-02 14:35:38,370] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-02 14:35:38,371] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-02 14:35:38,372] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-02 14:35:38,373] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:38,373] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:38,374] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:38,375] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:38,376] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:38,376] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:38,377] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:38,378] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:38,379] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:38,379] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:38,380] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:38,381] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:38,382] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:38,383] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:38,384] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:38,385] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:38,385] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:38,386] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:38,387] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:38,387] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:38,388] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:38,388] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:38,389] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:38,390] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-02 14:35:38,391] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-02 14:35:38,392] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:38,392] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-02 14:35:38,393] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-02 14:35:38,394] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-02 14:35:38,395] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-02 14:35:38,396] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-02 14:35:38,396] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-02 14:35:38,397] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:38,398] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:38,399] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:38,400] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:38,401] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-02 14:35:38,401] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-02 14:35:38,402] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-02 14:35:38,403] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-02 14:35:38,404] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-02 14:35:38,404] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-02 14:35:38,405] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:38,406] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:38,406] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:38,407] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:38,408] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:38,409] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:38,409] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:38,410] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:38,411] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:38,412] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:38,413] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-02 14:35:38,413] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-02 14:35:38,415] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-02 14:35:38,416] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-02 14:35:38,417] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-02 14:35:38,418] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-02 14:35:38,419] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:38,420] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:38,421] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:38,421] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:38,422] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:38,423] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 14:35:38,424] {spark_submit.py:495} INFO - 23/03/02 14:35:37 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-02 14:35:38,425] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:38,426] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:38,427] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:38,428] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-02 14:35:38,429] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-02 14:35:38,429] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-02 14:35:38,430] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-02 14:35:38,432] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-02 14:35:38,433] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-02 14:35:38,434] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:38,435] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-02 14:35:38,436] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-02 14:35:38,437] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:38,438] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-02 14:35:38,438] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-02 14:35:38,439] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-02 14:35:38,440] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-02 14:35:38,441] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-02 14:35:38,442] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:38,443] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:38,444] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:38,445] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-02 14:35:38,445] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-02 14:35:38,446] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-02 14:35:38,447] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-02 14:35:38,449] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-02 14:35:38,450] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-02 14:35:38,451] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-02 14:35:38,451] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-02 14:35:38,452] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-02 14:35:38,453] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:38,454] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:38,454] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:38,455] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:38,456] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:38,456] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:38,457] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-02 14:35:38,457] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 14:35:38,458] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1e8147149b8a:38967
[2023-03-02 14:35:38,459] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-02 14:35:38,460] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-02 14:35:38,460] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-02 14:35:38,461] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-02 14:35:38,462] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:38,462] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:38,463] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:38,464] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:38,465] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:38,466] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:38,467] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:38,467] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:38,468] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:38,469] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:38,470] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:38,471] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:38,471] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:38,472] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:38,473] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:38,473] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:38,474] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:38,475] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:38,475] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:38,476] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:38,476] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:38,477] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:38,477] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:38,478] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-02 14:35:38,479] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-02 14:35:38,479] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:38,480] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-02 14:35:38,480] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-02 14:35:38,482] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-02 14:35:38,482] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-02 14:35:38,483] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-02 14:35:38,484] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-02 14:35:38,485] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:38,485] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:38,486] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:38,486] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:38,487] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-02 14:35:38,488] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-02 14:35:38,488] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-02 14:35:38,489] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-02 14:35:38,489] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-02 14:35:38,490] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-02 14:35:38,491] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:38,491] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:38,492] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:38,493] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:38,493] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:38,494] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:38,494] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:38,495] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:38,495] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:38,496] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:38,496] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-02 14:35:38,497] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-02 14:35:38,498] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-02 14:35:38,499] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-02 14:35:38,500] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-02 14:35:38,501] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-02 14:35:38,502] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:38,502] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:38,503] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:38,504] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:38,504] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:38,505] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 14:35:38,506] {spark_submit.py:495} INFO - 23/03/02 14:35:37 INFO Executor: Told to re-register on heartbeat
[2023-03-02 14:35:38,506] {spark_submit.py:495} INFO - 23/03/02 14:35:37 INFO BlockManager: BlockManager BlockManagerId(driver, 1e8147149b8a, 32903, None) re-registering with master
[2023-03-02 14:35:38,507] {spark_submit.py:495} INFO - 23/03/02 14:35:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1e8147149b8a, 32903, None)
[2023-03-02 14:35:38,507] {spark_submit.py:495} INFO - 23/03/02 14:35:37 ERROR Inbox: Ignoring error
[2023-03-02 14:35:38,508] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:38,509] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:38,509] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:38,510] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-02 14:35:38,511] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-02 14:35:38,511] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-02 14:35:38,512] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-02 14:35:38,512] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-02 14:35:38,513] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-02 14:35:38,514] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-02 14:35:38,515] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-02 14:35:38,516] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-02 14:35:38,517] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:38,518] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:38,518] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:38,519] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:38,520] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:38,520] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:38,521] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-02 14:35:38,521] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-02 14:35:38,522] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-02 14:35:38,522] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-02 14:35:38,523] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1e8147149b8a:38967
[2023-03-02 14:35:38,524] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-02 14:35:38,524] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-02 14:35:38,525] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-02 14:35:38,525] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-02 14:35:38,526] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:38,526] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:38,527] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:38,527] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:38,528] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2023-03-02 14:35:38,529] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2023-03-02 14:35:38,529] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2023-03-02 14:35:38,530] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2023-03-02 14:35:38,530] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2023-03-02 14:35:38,531] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2023-03-02 14:35:38,532] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2023-03-02 14:35:38,533] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2023-03-02 14:35:38,533] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2023-03-02 14:35:38,534] {spark_submit.py:495} INFO - ... 19 more
[2023-03-02 14:35:38,535] {spark_submit.py:495} INFO - 23/03/02 14:35:37 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-02 14:35:38,536] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:38,536] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:38,537] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:38,537] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-02 14:35:38,538] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-02 14:35:38,538] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-02 14:35:38,539] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-02 14:35:38,540] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-02 14:35:38,540] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-02 14:35:38,541] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:38,541] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-02 14:35:38,542] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-02 14:35:38,542] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:38,543] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-02 14:35:38,543] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-02 14:35:38,544] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-02 14:35:38,545] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-02 14:35:38,545] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-02 14:35:38,546] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:38,546] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:38,547] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:38,548] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-02 14:35:38,549] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-02 14:35:38,550] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-02 14:35:38,550] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-02 14:35:38,551] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-02 14:35:38,551] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-02 14:35:38,552] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-02 14:35:38,552] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-02 14:35:38,553] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-02 14:35:38,553] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:38,554] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:38,554] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:38,555] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:38,555] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:38,556] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:38,556] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-02 14:35:38,557] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 14:35:38,557] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1e8147149b8a:38967
[2023-03-02 14:35:38,558] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-02 14:35:38,558] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-02 14:35:38,558] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-02 14:35:38,559] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-02 14:35:38,559] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:38,560] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:38,560] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:38,561] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:38,561] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2023-03-02 14:35:38,562] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2023-03-02 14:35:38,562] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2023-03-02 14:35:38,563] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2023-03-02 14:35:38,563] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2023-03-02 14:35:38,564] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2023-03-02 14:35:38,565] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2023-03-02 14:35:38,566] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2023-03-02 14:35:38,567] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2023-03-02 14:35:38,567] {spark_submit.py:495} INFO - ... 19 more
[2023-03-02 14:35:38,568] {spark_submit.py:495} INFO - 23/03/02 14:35:38 INFO Executor: Told to re-register on heartbeat
[2023-03-02 14:35:38,568] {spark_submit.py:495} INFO - 23/03/02 14:35:38 INFO BlockManager: BlockManager BlockManagerId(driver, 1e8147149b8a, 32903, None) re-registering with master
[2023-03-02 14:35:38,569] {spark_submit.py:495} INFO - 23/03/02 14:35:38 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1e8147149b8a, 32903, None)
[2023-03-02 14:35:39,480] {spark_submit.py:495} INFO - 23/03/02 14:35:38 ERROR Inbox: Ignoring error
[2023-03-02 14:35:40,121] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:40,122] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:40,123] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:40,124] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-02 14:35:40,124] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-02 14:35:40,125] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-02 14:35:40,126] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-02 14:35:40,126] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-02 14:35:40,127] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-02 14:35:40,128] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-02 14:35:40,128] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-02 14:35:40,129] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-02 14:35:40,130] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:40,131] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:40,132] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:40,133] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:40,134] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:40,134] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:40,135] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-02 14:35:40,136] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-02 14:35:40,137] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-02 14:35:40,137] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-02 14:35:40,138] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1e8147149b8a:38967
[2023-03-02 14:35:40,139] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-02 14:35:40,140] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-02 14:35:40,140] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-02 14:35:40,141] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-02 14:35:40,142] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:40,142] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:40,143] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:40,144] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:40,145] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:40,145] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:40,146] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:40,147] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:40,148] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:40,149] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:40,149] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:40,150] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:40,151] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:40,152] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:40,152] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:40,153] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:40,154] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:40,155] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:40,155] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:40,156] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:40,157] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:40,157] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:40,158] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:40,159] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-02 14:35:40,160] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-02 14:35:40,160] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:40,161] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-02 14:35:40,162] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-02 14:35:40,162] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-02 14:35:40,163] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-02 14:35:40,164] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-02 14:35:40,165] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-02 14:35:40,166] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:40,167] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:40,167] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:40,168] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:40,169] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-02 14:35:40,170] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-02 14:35:40,170] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-02 14:35:40,171] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-02 14:35:40,172] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-02 14:35:40,172] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-02 14:35:40,173] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:40,174] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:40,175] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:40,175] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:40,176] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:40,177] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:40,178] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:40,178] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:40,179] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:40,180] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:40,181] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-02 14:35:40,182] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-02 14:35:40,183] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-02 14:35:40,184] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-02 14:35:40,185] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-02 14:35:40,185] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-02 14:35:40,186] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:40,187] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:40,187] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:40,188] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:40,189] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:40,190] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 14:35:40,190] {spark_submit.py:495} INFO - 23/03/02 14:35:38 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-02 14:35:40,191] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:40,192] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:40,193] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:40,193] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-02 14:35:40,194] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-02 14:35:40,195] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-02 14:35:40,195] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-02 14:35:40,196] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-02 14:35:40,197] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-02 14:35:40,198] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:40,199] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-02 14:35:40,200] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-02 14:35:40,201] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:40,202] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-02 14:35:40,202] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-02 14:35:40,203] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-02 14:35:40,204] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-02 14:35:40,205] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-02 14:35:40,205] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:40,206] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:40,207] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:40,207] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-02 14:35:40,208] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-02 14:35:40,209] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-02 14:35:40,210] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-02 14:35:40,210] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-02 14:35:40,211] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-02 14:35:40,212] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-02 14:35:40,212] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-02 14:35:40,213] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-02 14:35:40,214] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:40,215] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:40,216] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:40,217] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:40,218] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:40,218] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:40,219] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-02 14:35:40,220] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 14:35:40,220] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1e8147149b8a:38967
[2023-03-02 14:35:40,221] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-02 14:35:40,222] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-02 14:35:40,222] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-02 14:35:40,223] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-02 14:35:40,224] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:40,225] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:40,225] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:40,226] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:40,227] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:40,227] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:40,228] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:40,229] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:40,229] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:40,230] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:40,231] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:40,232] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:40,233] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:40,234] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:40,235] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:40,235] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:40,236] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:40,237] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:40,238] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:40,238] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:40,239] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:40,240] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:40,241] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:40,241] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-02 14:35:40,242] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-02 14:35:40,243] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:40,243] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-02 14:35:40,244] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-02 14:35:40,245] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-02 14:35:40,245] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-02 14:35:40,246] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-02 14:35:40,247] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-02 14:35:40,248] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:40,249] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:40,250] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:40,251] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:40,252] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-02 14:35:40,253] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-02 14:35:40,253] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-02 14:35:40,254] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-02 14:35:40,255] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-02 14:35:40,256] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-02 14:35:40,256] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:40,257] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:40,258] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:40,258] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:40,259] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:40,260] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:40,260] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:40,261] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:40,262] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:40,262] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:40,263] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-02 14:35:40,264] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-02 14:35:40,265] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-02 14:35:40,266] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-02 14:35:40,267] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-02 14:35:40,268] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-02 14:35:40,268] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:40,269] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:40,270] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:40,270] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:40,271] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:40,272] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 14:35:40,525] {spark_submit.py:495} INFO - 23/03/02 14:35:39 INFO Executor: Told to re-register on heartbeat
[2023-03-02 14:35:40,770] {spark_submit.py:495} INFO - 23/03/02 14:35:40 INFO BlockManager: BlockManager BlockManagerId(driver, 1e8147149b8a, 32903, None) re-registering with master
[2023-03-02 14:35:40,771] {spark_submit.py:495} INFO - 23/03/02 14:35:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1e8147149b8a, 32903, None)
[2023-03-02 14:35:42,434] {spark_submit.py:495} INFO - 23/03/02 14:35:41 ERROR Inbox: Ignoring error
[2023-03-02 14:35:42,479] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:42,480] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:42,481] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:42,482] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-02 14:35:42,483] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-02 14:35:42,484] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-02 14:35:42,485] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-02 14:35:42,485] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-02 14:35:42,486] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-02 14:35:42,487] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-02 14:35:42,488] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-02 14:35:42,488] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-02 14:35:42,489] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:42,490] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:42,490] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:42,491] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:42,492] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:42,493] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:42,493] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-02 14:35:42,494] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-02 14:35:42,495] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-02 14:35:42,496] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-02 14:35:42,496] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1e8147149b8a:38967
[2023-03-02 14:35:42,498] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-02 14:35:42,499] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-02 14:35:42,500] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-02 14:35:42,500] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-02 14:35:42,501] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:42,502] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:42,503] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:42,503] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:42,504] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:42,505] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:42,505] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:42,506] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:42,507] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:42,507] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:42,508] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:42,509] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:42,509] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:42,510] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:42,511] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:42,511] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:42,512] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:42,513] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:42,514] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:42,515] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:42,516] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:42,516] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:42,517] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:42,518] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-02 14:35:42,519] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-02 14:35:42,519] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:42,520] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-02 14:35:42,521] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-02 14:35:42,521] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-02 14:35:42,522] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-02 14:35:42,523] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-02 14:35:42,523] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-02 14:35:42,524] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:42,525] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:42,525] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:42,526] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:42,527] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-02 14:35:42,527] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-02 14:35:42,528] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-02 14:35:42,528] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-02 14:35:42,529] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-02 14:35:42,530] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-02 14:35:42,531] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:42,532] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:42,533] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:42,533] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:42,534] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:42,535] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:42,535] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:42,536] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:42,537] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:42,537] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:42,538] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-02 14:35:42,539] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-02 14:35:42,540] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-02 14:35:42,540] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-02 14:35:42,541] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-02 14:35:42,542] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-02 14:35:42,542] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:42,543] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:42,543] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:42,544] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:42,545] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:42,545] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 14:35:42,546] {spark_submit.py:495} INFO - 23/03/02 14:35:41 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-02 14:35:42,547] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:42,548] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:42,549] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:42,550] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-02 14:35:42,550] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-02 14:35:42,551] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-02 14:35:42,552] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-02 14:35:42,553] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-02 14:35:42,553] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-02 14:35:42,554] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:42,555] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-02 14:35:42,555] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-02 14:35:42,556] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:42,557] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-02 14:35:42,557] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-02 14:35:42,558] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-02 14:35:42,559] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-02 14:35:42,559] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-02 14:35:42,560] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:42,561] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:42,561] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:42,562] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-02 14:35:42,563] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-02 14:35:42,564] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-02 14:35:42,565] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-02 14:35:42,565] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-02 14:35:42,566] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-02 14:35:42,567] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-02 14:35:42,568] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-02 14:35:42,568] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-02 14:35:42,569] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:42,570] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:42,570] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:42,571] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:42,572] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:42,572] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:42,573] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-02 14:35:42,574] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 14:35:42,574] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1e8147149b8a:38967
[2023-03-02 14:35:42,575] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-02 14:35:42,576] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-02 14:35:42,577] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-02 14:35:42,577] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-02 14:35:42,578] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:42,579] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:42,579] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:42,580] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:42,581] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:42,582] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:42,583] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:42,584] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:42,585] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:42,586] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:42,586] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:42,587] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:42,588] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:42,588] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:42,589] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:42,590] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:42,590] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:42,591] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:42,592] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:42,592] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:42,593] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:42,594] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:42,594] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:42,595] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-02 14:35:42,595] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-02 14:35:42,596] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:42,597] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-02 14:35:42,598] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-02 14:35:42,599] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-02 14:35:42,600] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-02 14:35:42,600] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-02 14:35:42,601] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-02 14:35:42,602] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:42,603] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:42,603] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:42,604] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:42,605] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-02 14:35:42,605] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-02 14:35:42,606] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-02 14:35:42,607] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-02 14:35:42,608] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-02 14:35:42,608] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-02 14:35:42,609] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:42,610] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:42,610] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:42,611] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:42,612] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:42,612] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:42,613] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:42,614] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:42,615] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:42,616] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:42,617] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-02 14:35:42,618] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-02 14:35:42,618] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-02 14:35:42,619] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-02 14:35:42,620] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-02 14:35:42,620] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-02 14:35:42,621] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:42,622] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:42,623] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:42,623] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:42,624] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:42,625] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 14:35:43,086] {spark_submit.py:495} INFO - 23/03/02 14:35:42 INFO Executor: Told to re-register on heartbeat
[2023-03-02 14:35:43,263] {spark_submit.py:495} INFO - 23/03/02 14:35:43 INFO BlockManager: BlockManager BlockManagerId(driver, 1e8147149b8a, 32903, None) re-registering with master
[2023-03-02 14:35:43,265] {spark_submit.py:495} INFO - 23/03/02 14:35:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1e8147149b8a, 32903, None)
[2023-03-02 14:35:44,543] {spark_submit.py:495} INFO - 23/03/02 14:35:44 ERROR Inbox: Ignoring error
[2023-03-02 14:35:44,755] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:44,755] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:44,756] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:44,757] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-02 14:35:44,758] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-02 14:35:44,758] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-02 14:35:44,759] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-02 14:35:44,760] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-02 14:35:44,761] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-02 14:35:44,761] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-02 14:35:44,762] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-02 14:35:44,763] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-02 14:35:44,764] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:44,765] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:44,765] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:44,766] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:44,767] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:44,768] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:44,768] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-02 14:35:44,769] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-02 14:35:44,770] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-02 14:35:44,770] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-02 14:35:44,771] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1e8147149b8a:38967
[2023-03-02 14:35:44,772] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-02 14:35:44,773] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-02 14:35:44,773] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-02 14:35:44,774] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-02 14:35:44,775] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:44,775] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:44,776] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:44,777] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:44,777] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2023-03-02 14:35:44,778] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2023-03-02 14:35:44,779] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2023-03-02 14:35:44,780] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2023-03-02 14:35:44,781] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2023-03-02 14:35:44,782] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2023-03-02 14:35:44,783] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2023-03-02 14:35:44,784] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2023-03-02 14:35:44,784] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2023-03-02 14:35:44,785] {spark_submit.py:495} INFO - ... 19 more
[2023-03-02 14:35:44,786] {spark_submit.py:495} INFO - 23/03/02 14:35:44 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-02 14:35:44,786] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:44,787] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:44,788] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:44,789] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-02 14:35:44,789] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-02 14:35:44,790] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-02 14:35:44,791] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-02 14:35:44,791] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-02 14:35:44,792] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-02 14:35:44,793] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:44,793] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-02 14:35:44,794] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-02 14:35:44,795] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:44,796] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-02 14:35:44,797] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-02 14:35:44,798] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-02 14:35:44,799] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-02 14:35:44,800] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-02 14:35:44,800] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:44,801] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:44,802] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:44,803] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-02 14:35:44,803] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-02 14:35:44,804] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-02 14:35:44,805] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-02 14:35:44,805] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-02 14:35:44,806] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-02 14:35:44,807] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-02 14:35:44,807] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-02 14:35:44,808] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-02 14:35:44,809] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:44,810] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:44,810] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:44,811] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:44,812] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:44,813] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:44,814] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-02 14:35:44,815] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 14:35:44,816] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1e8147149b8a:38967
[2023-03-02 14:35:44,817] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-02 14:35:44,818] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-02 14:35:44,818] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-02 14:35:44,819] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-02 14:35:44,820] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:44,821] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:44,821] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:44,822] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:44,823] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2023-03-02 14:35:44,824] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2023-03-02 14:35:44,824] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2023-03-02 14:35:44,825] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2023-03-02 14:35:44,826] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2023-03-02 14:35:44,827] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2023-03-02 14:35:44,827] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2023-03-02 14:35:44,828] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2023-03-02 14:35:44,829] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2023-03-02 14:35:44,831] {spark_submit.py:495} INFO - ... 19 more
[2023-03-02 14:35:47,178] {spark_submit.py:495} INFO - 23/03/02 14:35:46 INFO Executor: Told to re-register on heartbeat
[2023-03-02 14:35:47,895] {spark_submit.py:495} INFO - 23/03/02 14:35:47 INFO BlockManager: BlockManager BlockManagerId(driver, 1e8147149b8a, 32903, None) re-registering with master
[2023-03-02 14:35:48,253] {spark_submit.py:495} INFO - 23/03/02 14:35:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1e8147149b8a, 32903, None)
[2023-03-02 14:35:50,191] {spark_submit.py:495} INFO - 23/03/02 14:35:48 ERROR Inbox: Ignoring error
[2023-03-02 14:35:50,280] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:50,300] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:50,301] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:50,301] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-02 14:35:50,302] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-02 14:35:50,303] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-02 14:35:50,303] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-02 14:35:50,304] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-02 14:35:50,304] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-02 14:35:50,305] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-02 14:35:50,306] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-02 14:35:50,306] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-02 14:35:50,307] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:50,308] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:50,308] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:50,309] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:50,310] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:50,311] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:50,311] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-02 14:35:50,312] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-02 14:35:50,313] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-02 14:35:50,314] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-02 14:35:50,315] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1e8147149b8a:38967
[2023-03-02 14:35:50,316] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-02 14:35:50,317] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-02 14:35:50,318] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-02 14:35:50,318] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-02 14:35:50,319] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:50,319] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:50,320] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:50,321] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:50,322] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:50,322] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:50,323] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:50,324] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:50,324] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:50,325] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:50,326] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:50,326] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:50,327] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:50,328] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:50,329] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:50,330] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:50,331] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:50,331] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:50,332] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:50,333] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:50,334] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:50,334] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:50,335] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:50,336] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-02 14:35:50,336] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-02 14:35:50,337] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:50,338] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-02 14:35:50,338] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-02 14:35:50,339] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-02 14:35:50,340] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-02 14:35:50,340] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-02 14:35:50,341] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-02 14:35:50,342] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:50,342] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:50,343] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:50,344] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:50,344] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-02 14:35:50,345] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-02 14:35:50,346] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-02 14:35:50,347] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-02 14:35:50,348] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-02 14:35:50,349] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-02 14:35:50,349] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:50,350] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:50,351] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:50,351] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:50,352] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:50,353] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:50,353] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:50,354] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:50,355] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:50,355] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:50,356] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-02 14:35:50,357] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-02 14:35:50,358] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-02 14:35:50,358] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-02 14:35:50,359] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-02 14:35:50,360] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-02 14:35:50,361] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:50,361] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:50,362] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:50,363] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:50,364] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:50,365] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 14:35:50,366] {spark_submit.py:495} INFO - 23/03/02 14:35:48 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-02 14:35:50,366] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:50,367] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:50,368] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:50,368] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-02 14:35:50,369] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-02 14:35:50,370] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-02 14:35:50,371] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-02 14:35:50,371] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-02 14:35:50,372] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-02 14:35:50,373] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:50,373] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-02 14:35:50,374] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-02 14:35:50,375] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:50,376] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-02 14:35:50,377] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-02 14:35:50,377] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-02 14:35:50,378] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-02 14:35:50,379] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-02 14:35:50,380] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:50,381] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:50,382] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:50,382] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-02 14:35:50,383] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-02 14:35:50,384] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-02 14:35:50,385] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-02 14:35:50,385] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-02 14:35:50,386] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-02 14:35:50,387] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-02 14:35:50,388] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-02 14:35:50,388] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-02 14:35:50,389] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:50,390] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:50,391] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:50,391] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:50,392] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:50,393] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:50,394] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-02 14:35:50,394] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 14:35:50,395] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1e8147149b8a:38967
[2023-03-02 14:35:50,396] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-02 14:35:50,397] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-02 14:35:50,398] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-02 14:35:50,399] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-02 14:35:50,400] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:50,400] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:50,401] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:50,402] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:50,402] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:50,403] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:50,404] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:50,404] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:50,405] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:50,406] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:50,407] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:50,408] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:50,408] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:50,409] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:50,410] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:50,411] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:50,411] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:50,412] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:50,413] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:50,414] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:50,415] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:50,416] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:50,416] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:50,417] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-02 14:35:50,418] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-02 14:35:50,418] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:50,419] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-02 14:35:50,420] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-02 14:35:50,421] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-02 14:35:50,422] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-02 14:35:50,422] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-02 14:35:50,423] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-02 14:35:50,424] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:50,424] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:50,425] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:50,426] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:50,427] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-02 14:35:50,427] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-02 14:35:50,428] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-02 14:35:50,429] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-02 14:35:50,430] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-02 14:35:50,431] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-02 14:35:50,432] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:50,433] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:50,433] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:50,434] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:50,435] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:50,436] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:50,436] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:50,437] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:50,437] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:50,438] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:50,439] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-02 14:35:50,440] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-02 14:35:50,440] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-02 14:35:50,441] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-02 14:35:50,442] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-02 14:35:50,442] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-02 14:35:50,443] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:50,444] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:50,445] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:50,446] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:50,447] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:50,448] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 14:35:51,680] {spark_submit.py:495} INFO - 23/03/02 14:35:51 INFO Executor: Told to re-register on heartbeat
[2023-03-02 14:35:52,497] {spark_submit.py:495} INFO - 23/03/02 14:35:51 INFO BlockManager: BlockManager BlockManagerId(driver, 1e8147149b8a, 32903, None) re-registering with master
[2023-03-02 14:35:52,627] {spark_submit.py:495} INFO - 23/03/02 14:35:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1e8147149b8a, 32903, None)
[2023-03-02 14:35:54,468] {spark_submit.py:495} INFO - 23/03/02 14:35:53 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-02 14:35:54,627] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:54,629] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:54,629] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:54,630] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-02 14:35:54,631] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-02 14:35:54,632] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-02 14:35:54,632] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-02 14:35:54,633] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-02 14:35:54,634] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-02 14:35:54,635] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:54,635] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-02 14:35:54,636] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-02 14:35:54,637] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:54,637] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-02 14:35:54,638] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-02 14:35:54,639] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-02 14:35:54,639] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-02 14:35:54,640] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-02 14:35:54,641] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:54,642] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:54,642] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:54,643] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-02 14:35:54,644] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-02 14:35:54,645] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-02 14:35:54,646] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-02 14:35:54,647] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-02 14:35:54,647] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-02 14:35:54,648] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-02 14:35:54,649] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-02 14:35:54,650] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-02 14:35:54,650] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:54,651] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:54,652] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:54,653] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:54,654] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:54,655] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:54,655] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-02 14:35:54,656] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 14:35:54,657] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1e8147149b8a:38967
[2023-03-02 14:35:54,657] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-02 14:35:54,658] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-02 14:35:54,659] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-02 14:35:54,659] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-02 14:35:54,660] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:54,661] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:54,662] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:54,663] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:54,664] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:54,665] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:54,665] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:54,666] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:54,667] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:54,667] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:54,668] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:54,669] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:54,669] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:54,670] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:54,671] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:54,671] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:54,672] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:54,673] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:54,674] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:54,674] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:54,675] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:54,676] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:54,677] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:54,677] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-02 14:35:54,678] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-02 14:35:54,679] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:54,680] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-02 14:35:54,681] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-02 14:35:54,682] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-02 14:35:54,682] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-02 14:35:54,683] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-02 14:35:54,684] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-02 14:35:54,684] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:54,685] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:54,686] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:54,687] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:54,687] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-02 14:35:54,688] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-02 14:35:54,689] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-02 14:35:54,689] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-02 14:35:54,690] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-02 14:35:54,691] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-02 14:35:54,691] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:54,692] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:54,693] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:54,693] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:54,694] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:54,695] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:54,696] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:54,697] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:54,698] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:54,699] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:54,699] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-02 14:35:54,700] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-02 14:35:54,701] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-02 14:35:54,701] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-02 14:35:54,702] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-02 14:35:54,703] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-02 14:35:54,703] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:54,704] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:54,705] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:54,705] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:54,706] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:54,707] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 14:35:54,708] {spark_submit.py:495} INFO - 23/03/02 14:35:53 ERROR Inbox: Ignoring error
[2023-03-02 14:35:54,708] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:54,709] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:54,710] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:54,710] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-02 14:35:54,712] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-02 14:35:54,713] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-02 14:35:54,713] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-02 14:35:54,714] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-02 14:35:54,715] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-02 14:35:54,716] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-02 14:35:54,716] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-02 14:35:54,717] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-02 14:35:54,718] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:54,719] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:54,719] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:54,720] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:54,721] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:54,721] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:54,722] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-02 14:35:54,723] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-02 14:35:54,724] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-02 14:35:54,724] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-02 14:35:54,725] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1e8147149b8a:38967
[2023-03-02 14:35:54,726] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-02 14:35:54,726] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-02 14:35:54,727] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-02 14:35:54,728] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-02 14:35:54,729] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:54,730] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:54,730] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:54,731] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:54,732] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:54,733] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:54,734] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:54,734] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:54,735] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:54,736] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:54,736] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:54,737] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:54,738] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:54,738] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:54,739] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:54,740] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:54,740] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:54,741] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:54,742] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:54,742] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:54,743] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:54,744] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-02 14:35:54,745] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:54,746] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-02 14:35:54,747] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-02 14:35:54,748] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:54,748] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-02 14:35:54,749] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-02 14:35:54,750] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-02 14:35:54,751] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-02 14:35:54,751] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-02 14:35:54,752] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-02 14:35:54,753] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:54,753] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:54,754] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:54,755] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:54,755] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-02 14:35:54,756] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-02 14:35:54,757] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-02 14:35:54,757] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-02 14:35:54,758] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-02 14:35:54,759] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-02 14:35:54,760] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:54,760] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:54,762] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:54,763] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:54,764] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-02 14:35:54,764] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-02 14:35:54,765] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-02 14:35:54,766] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-02 14:35:54,767] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-02 14:35:54,767] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-02 14:35:54,768] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-02 14:35:54,769] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-02 14:35:54,769] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-02 14:35:54,770] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-02 14:35:54,771] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-02 14:35:54,772] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-02 14:35:54,772] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:54,773] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:54,774] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:54,774] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:54,775] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:54,776] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 14:35:55,840] {spark_submit.py:495} INFO - 23/03/02 14:35:55 INFO Executor: Told to re-register on heartbeat
[2023-03-02 14:35:56,207] {spark_submit.py:495} INFO - 23/03/02 14:35:55 INFO BlockManager: BlockManager BlockManagerId(driver, 1e8147149b8a, 32903, None) re-registering with master
[2023-03-02 14:35:56,830] {spark_submit.py:495} INFO - 23/03/02 14:35:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1e8147149b8a, 32903, None)
[2023-03-02 14:35:59,656] {spark_submit.py:495} INFO - 23/03/02 14:35:57 ERROR Inbox: Ignoring error
[2023-03-02 14:35:59,808] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:59,808] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:59,809] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:59,810] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-02 14:35:59,811] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-02 14:35:59,812] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-02 14:35:59,813] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-02 14:35:59,813] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-02 14:35:59,814] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-02 14:35:59,815] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-02 14:35:59,816] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-02 14:35:59,816] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-02 14:35:59,817] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:59,818] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:59,818] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:59,819] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:59,820] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:59,820] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:59,821] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-02 14:35:59,822] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-02 14:35:59,822] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-02 14:35:59,823] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-02 14:35:59,824] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1e8147149b8a:38967
[2023-03-02 14:35:59,824] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-02 14:35:59,825] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-02 14:35:59,826] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-02 14:35:59,827] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-02 14:35:59,828] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:59,828] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:59,829] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:59,830] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:59,831] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2023-03-02 14:35:59,831] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2023-03-02 14:35:59,832] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2023-03-02 14:35:59,832] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2023-03-02 14:35:59,833] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2023-03-02 14:35:59,834] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2023-03-02 14:35:59,834] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2023-03-02 14:35:59,835] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2023-03-02 14:35:59,835] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2023-03-02 14:35:59,836] {spark_submit.py:495} INFO - ... 19 more
[2023-03-02 14:35:59,837] {spark_submit.py:495} INFO - 23/03/02 14:35:57 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-02 14:35:59,837] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:59,838] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:59,839] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:59,839] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-02 14:35:59,840] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-02 14:35:59,840] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-02 14:35:59,841] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-02 14:35:59,842] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-02 14:35:59,842] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-02 14:35:59,843] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 14:35:59,844] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-02 14:35:59,845] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-02 14:35:59,846] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:59,847] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-02 14:35:59,847] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-02 14:35:59,848] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-02 14:35:59,849] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-02 14:35:59,849] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-02 14:35:59,850] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:35:59,851] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:35:59,852] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:35:59,852] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-02 14:35:59,853] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-02 14:35:59,854] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-02 14:35:59,854] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-02 14:35:59,855] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-02 14:35:59,856] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-02 14:35:59,856] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-02 14:35:59,857] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-02 14:35:59,858] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-02 14:35:59,858] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 14:35:59,859] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 14:35:59,860] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 14:35:59,861] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 14:35:59,862] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 14:35:59,863] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 14:35:59,863] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-02 14:35:59,864] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 14:35:59,865] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1e8147149b8a:38967
[2023-03-02 14:35:59,866] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-02 14:35:59,866] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-02 14:35:59,867] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-02 14:35:59,868] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-02 14:35:59,869] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 14:35:59,869] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 14:35:59,870] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 14:35:59,870] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 14:35:59,871] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2023-03-02 14:35:59,872] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2023-03-02 14:35:59,873] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2023-03-02 14:35:59,873] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2023-03-02 14:35:59,874] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2023-03-02 14:35:59,875] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2023-03-02 14:35:59,875] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2023-03-02 14:35:59,876] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2023-03-02 14:35:59,877] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2023-03-02 14:35:59,878] {spark_submit.py:495} INFO - ... 19 more
[2023-03-02 14:36:03,279] {spark_submit.py:495} INFO - 23/03/02 14:36:02 INFO Executor: Told to re-register on heartbeat
[2023-03-02 14:36:03,626] {spark_submit.py:495} INFO - 23/03/02 14:36:03 INFO BlockManager: BlockManager BlockManagerId(driver, 1e8147149b8a, 32903, None) re-registering with master
[2023-03-02 14:36:04,481] {spark_submit.py:495} INFO - 23/03/02 14:36:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1e8147149b8a, 32903, None)
[2023-03-02 14:41:21,766] {spark_submit.py:495} INFO - 23/03/02 14:37:30 ERROR Inbox: Ignoring error
[2023-03-02 14:42:35,658] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 14:44:46,274] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 14:48:29,103] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 14:49:44,868] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-02 14:50:45,420] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-02 14:52:34,807] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-02 14:53:47,881] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-02 15:00:07,663] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-02 15:26:55,584] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-02 15:27:08,965] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-02 15:27:22,405] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-02 15:27:41,029] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-02 15:27:48,569] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 15:28:07,531] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 15:28:37,604] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 15:28:43,784] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 15:29:08,382] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 15:29:30,138] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 15:29:38,521] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-02 15:29:38,644] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-02 15:29:38,645] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-02 15:29:38,645] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-02 15:29:38,646] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1e8147149b8a:38967
[2023-03-02 15:29:38,647] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-02 15:29:38,648] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-02 15:29:38,649] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-02 15:29:38,650] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-02 15:29:38,651] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 15:29:38,652] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 15:29:38,652] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 15:29:38,653] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 15:29:38,653] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2023-03-02 15:29:38,654] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2023-03-02 15:29:38,655] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2023-03-02 15:29:38,655] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2023-03-02 15:29:38,656] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2023-03-02 15:29:38,656] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2023-03-02 15:29:38,658] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2023-03-02 15:29:38,658] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2023-03-02 15:29:38,659] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2023-03-02 15:29:38,660] {spark_submit.py:495} INFO - ... 19 more
[2023-03-02 15:29:38,660] {spark_submit.py:495} INFO - 23/03/02 14:37:30 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-02 15:29:38,662] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 15:29:38,662] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 15:29:38,663] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 15:29:38,663] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-02 15:29:38,664] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-02 15:29:38,666] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-02 15:29:38,667] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-02 15:29:38,668] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-02 15:29:38,668] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-02 15:29:38,669] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-02 15:29:38,669] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-02 15:29:38,670] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-02 15:29:38,671] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 15:29:38,671] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-02 15:29:38,672] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-02 15:29:38,673] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-02 15:29:38,673] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-02 15:29:38,674] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-02 15:29:38,674] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-02 15:29:38,675] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-02 15:29:38,676] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-02 15:29:38,677] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-02 15:29:38,677] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-02 15:29:38,678] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-02 15:29:38,678] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-02 15:29:38,679] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-02 15:29:38,680] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-02 15:29:38,681] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-02 15:29:38,682] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-02 15:29:38,684] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-02 15:29:38,684] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-02 15:29:38,685] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-02 15:29:38,686] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-02 15:29:38,686] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-02 15:29:38,687] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-02 15:29:38,687] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-02 15:29:38,688] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-02 15:29:38,689] {spark_submit.py:495} INFO - ... 3 more
[2023-03-02 15:29:38,689] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1e8147149b8a:38967
[2023-03-02 15:29:38,690] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-02 15:29:38,691] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-02 15:29:38,693] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-02 15:29:38,694] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-02 15:29:38,694] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-02 15:29:38,696] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-02 15:29:38,697] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-02 15:29:38,701] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-02 15:29:38,702] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2023-03-02 15:29:38,703] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2023-03-02 15:29:38,703] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2023-03-02 15:29:38,704] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2023-03-02 15:29:38,707] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2023-03-02 15:29:38,708] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2023-03-02 15:29:38,710] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2023-03-02 15:29:38,711] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2023-03-02 15:29:38,712] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2023-03-02 15:29:38,713] {spark_submit.py:495} INFO - ... 19 more
[2023-03-02 15:29:38,714] {spark_submit.py:495} INFO - 23/03/02 14:41:12 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@57081e8b)) by listener ExecutionListenerBus took 1.390231988s.
[2023-03-02 15:29:38,716] {spark_submit.py:495} INFO - 23/03/02 14:41:35 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@57081e8b)) by listener HeartbeatReceiver took 2.973010473s.
[2023-03-02 15:29:38,718] {spark_submit.py:495} INFO - 23/03/02 14:42:43 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@57081e8b)) by listener AppStatusListener took 42.500240356s.
[2023-03-02 15:29:38,719] {spark_submit.py:495} INFO - 23/03/02 15:27:08 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@57081e8b)) by listener SQLAppStatusListener took 13.195895037s.
[2023-03-02 15:29:39,259] {local_task_job.py:144} ERROR - Heartbeat time limit exceeded!
[2023-03-02 15:29:39,323] {process_utils.py:129} INFO - Sending Signals.SIGTERM to group 478. PIDs of all processes in the group: [479, 532, 478]
[2023-03-02 15:29:39,324] {process_utils.py:80} INFO - Sending the signal Signals.SIGTERM to group 478
[2023-03-02 15:29:39,325] {taskinstance.py:1541} ERROR - Received SIGTERM. Terminating subprocesses.
[2023-03-02 15:29:39,327] {spark_submit.py:620} INFO - Sending kill signal to spark-submit
[2023-03-02 15:29:40,067] {taskinstance.py:1889} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 414, in submit
    self._process_spark_submit_log(iter(self._submit_sp.stdout))  # type: ignore
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 463, in _process_spark_submit_log
    for line in itr:
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1543, in signal_handler
    raise AirflowException("Task received SIGTERM signal")
airflow.exceptions.AirflowException: Task received SIGTERM signal
[2023-03-02 15:29:40,262] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=avg_product_price, task_id=spark_submit, execution_date=20230302T142209, start_date=20230302T142813, end_date=20230302T152940
[2023-03-02 15:29:40,373] {standard_task_runner.py:97} ERROR - Failed to execute job 45 for task spark_submit (Task received SIGTERM signal; 478)
[2023-03-02 15:29:41,024] {process_utils.py:75} INFO - Process psutil.Process(pid=478, status='terminated', exitcode=1, started='14:28:13') (478) terminated with exit code 1
[2023-03-02 15:30:39,361] {process_utils.py:143} WARNING - process psutil.Process(pid=532, name='python3', status='zombie', started='14:28:26') did not respond to SIGTERM. Trying SIGKILL
[2023-03-02 15:30:39,367] {process_utils.py:143} WARNING - process psutil.Process(pid=479, name='java', status='zombie', started='14:28:13') did not respond to SIGTERM. Trying SIGKILL
[2023-03-02 15:30:39,367] {process_utils.py:80} INFO - Sending the signal Signals.SIGKILL to group 478
[2023-03-02 15:31:39,423] {process_utils.py:154} ERROR - Process psutil.Process(pid=532, name='python3', status='zombie', started='14:28:26') (532) could not be killed. Giving up.
[2023-03-02 15:31:39,452] {process_utils.py:154} ERROR - Process psutil.Process(pid=479, name='java', status='zombie', started='14:28:13') (479) could not be killed. Giving up.
